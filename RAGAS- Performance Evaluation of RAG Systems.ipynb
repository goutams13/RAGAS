{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfe37963-1af6-44fc-a841-8e462443f5e6",
   "metadata": {},
   "source": [
    "# RAGAS- Performance Evaluation of RAG Systems\n",
    "\n",
    "## InsureLLM Company Question Answering CHATBOT\n",
    "This project builds a low cost, high accuracy question answering system designed for employees of InsureLLM, an Insurance Tech company. The chatbot acts as an expert knowledge worker, helping staff quickly find accurate answers to domain specific queries. To achieve reliability, the system leverages Retrieval-Augmented Generation (RAG), combining document retrieval with LLM reasoning. This ensures responses are context-grounded, relevant, and scalable for enterprise use.\n",
    "\n",
    "This Project integrates RAGAS metrics (faithfulness, relevancy, precision, recall, correctness) to automatically assess answer quality, and saves detailed results for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b9e87b-c756-4f11-81d9-350fe9c64b71",
   "metadata": {},
   "source": [
    "### Installing the Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b062a737-3f6b-4cca-bad0-c62c68411169",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-huggingface in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (0.3.1)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.70 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from langchain-huggingface) (0.3.74)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from langchain-huggingface) (0.21.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.33.4 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from langchain-huggingface) (0.34.4)\n",
      "Requirement already satisfied: langsmith>=0.3.45 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.3.45)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (4.13.2)\n",
      "Requirement already satisfied: packaging>=23.2 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (24.2)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (2.11.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (3.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (2025.3.0)\n",
      "Requirement already satisfied: requests in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (4.67.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.23.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (4.9.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.4.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from requests->huggingface-hub>=0.33.4->langchain-huggingface) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from requests->huggingface-hub>=0.33.4->langchain-huggingface) (2.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.33.4->langchain-huggingface) (0.4.6)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (1.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U langchain-huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da73b18-8621-4abc-bd50-58794883f49d",
   "metadata": {},
   "source": [
    "### Importing The Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba2779af-84ef-4227-9e9e-6eaf0df87e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "802137aa-8a74-45e0-a487-d1974927d7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.graph_objects as go\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.llms import HuggingFaceHub\n",
    "from langchain_huggingface import HuggingFaceEndpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58c85082-e417-4708-9efe-81a5d55d1424",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "db_name = \"vector_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee78efcb-60fe-449e-a944-40bab26261af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables in a file called .env\n",
    "\n",
    "load_dotenv(override=True)\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f829cc35-00aa-44cc-b282-269bed0397b5",
   "metadata": {},
   "source": [
    "### Reading The Document from Our Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "730711a9-6ffe-4eee-8f48-d6cfb7314905",
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = glob.glob(\"knowledge-base/*\")\n",
    "text_loader_kwargs = {'encoding': 'utf-8'}\n",
    "\n",
    "documents = []\n",
    "for folder in folders:\n",
    "    doc_type = os.path.basename(folder)\n",
    "    loader = DirectoryLoader(folder, glob=\"**/*.md\", loader_cls=TextLoader, loader_kwargs=text_loader_kwargs)\n",
    "    folder_docs = loader.load()\n",
    "    for doc in folder_docs:\n",
    "        doc.metadata[\"doc_type\"] = doc_type\n",
    "        documents.append(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424ae2a0-8a4d-4679-9b2f-186cd569cf37",
   "metadata": {},
   "source": [
    "### Creating Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7310c9c8-03c1-4efc-a104-5e89aec6db1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1088, which is longer than the specified 1000\n"
     ]
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd06e02f-6d9b-44cc-a43d-e1faa8acc7bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904b18cc-ed34-4d83-b728-00c039a1fd28",
   "metadata": {},
   "source": [
    "### Documents in Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c54b4b6-06da-463d-bee7-4dd456c2b887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document types found: company, employees, contracts, products\n"
     ]
    }
   ],
   "source": [
    "doc_types = set(chunk.metadata['doc_type'] for chunk in chunks)\n",
    "print(f\"Document types found: {', '.join(doc_types)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f7d2a6-ccfa-425b-a1c3-5e55b23bd013",
   "metadata": {},
   "source": [
    "### Embeddings, and \"Auto-Encoding LLMs\"\n",
    "\n",
    "We will be mapping each chunk of text into a Vector that represents the meaning of the text, known as an embedding.\n",
    "This model is an example of an \"Auto-Encoding LLM\" which generates an output given a complete input.\n",
    "Another example of an Auto-Encoding LLMs is BERT from Google. In addition to embedding, Auto-encoding LLMs are often used for classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78998399-ac17-4e28-b15f-0b5f51e6ee23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GoutamSahu\\AppData\\Local\\Temp\\ipykernel_23620\\2150730444.py:11: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorstore created with 123 documents\n"
     ]
    }
   ],
   "source": [
    "# Put the chunks of data into a Vector Store that associates a Vector Embedding with each chunk\n",
    "# Chroma is a popular open source Vector Database based on SQLLite\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Delete if already exists\n",
    "\n",
    "if os.path.exists(db_name):\n",
    "    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()\n",
    "\n",
    "# Create vectorstore\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)\n",
    "print(f\"Vectorstore created with {vectorstore._collection.count()} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "057868f6-51a6-4087-94d1-380145821550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vectors have 384 dimensions\n"
     ]
    }
   ],
   "source": [
    "# Get one vector and find how many dimensions it has\n",
    "\n",
    "collection = vectorstore._collection\n",
    "sample_embedding = collection.get(limit=1, include=[\"embeddings\"])[\"embeddings\"][0]\n",
    "dimensions = len(sample_embedding)\n",
    "print(f\"The vectors have {dimensions:,} dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9468860b-86a2-41df-af01-b2400cc985be",
   "metadata": {},
   "source": [
    "## RAG Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5dbacb42-86df-460d-828e-7d78c6180b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GoutamSahu\\AppData\\Local\\Temp\\ipykernel_23620\\1458413148.py:19: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Insurellm is an innovative insurance tech startup founded by Avery Lancaster in 2015. It offers a range of services including AI-powered risk assessment, dynamic pricing, instant claim processing, predictive maintenance alerts, multi-channel integration, a customer portal, and comprehensive support for effective onboarding and 24/7 technical assistance. Insurellm aims to transform the home insurance landscape by combining innovation and reliability.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "\n",
    "# Hugging Face Endpoint (Mistral Instruct = conversational model)\n",
    "endpoint = HuggingFaceEndpoint(\n",
    "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    task=\"conversational\",\n",
    "    temperature=0.7,\n",
    "    max_new_tokens=512,\n",
    "    huggingfacehub_api_token=os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    ")\n",
    "\n",
    "# Wrap in Chat interface\n",
    "llm = ChatHuggingFace(llm=endpoint)\n",
    "\n",
    "# Memory\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    "    output_key=\"answer\" \n",
    ")\n",
    "\n",
    "# Retriever (from vectorstore)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Conversation Chain\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    return_source_documents=True,\n",
    "    output_key=\"answer\" \n",
    ")\n",
    "\n",
    "# Test Query\n",
    "query = \"Can you describe Insurellm in a few sentences?\"\n",
    "result = conversation_chain.invoke({\"question\": query})\n",
    "print(result[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383de303-e3f1-413e-9d96-5604e1784263",
   "metadata": {},
   "source": [
    "### Sample Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "630c6e35-4389-4e45-b119-3cc0f09ffe77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Insurellm is an insurance tech startup founded by Avery Lancaster in 2015. It was designed to disrupt the insurance industry with innovative products. Their first product was Markellm, a marketplace connecting consumers with insurance providers. By 2024, Insurellm had expanded to 200 employees and 12 offices across the US. They offer ongoing updates and enhancements to their Homellm platform, including new features and security improvements, and they actively solicit feedback from their clients to ensure their products continue to meet their evolving needs. They provide 24/7 technical support via email and phone assistance for the duration of their contracts.\n"
     ]
    }
   ],
   "source": [
    "query = \"Can you describe Insurellm in a few sentences?\"\n",
    "result = conversation_chain.invoke({\"question\": query})\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6c5d5f-851d-4d83-a306-87776d0f6237",
   "metadata": {},
   "source": [
    "### Setting Up Conversation Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6eb99fb-33ec-4025-ab92-b634ede03647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a new conversation memory for the chat\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "# putting it together: set up the conversation chain with the LLM, the vector store and memory\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbcb659-13ce-47ab-8a5e-01b930494964",
   "metadata": {},
   "source": [
    "### Gradio Chatbot View "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c3536590-85c7-4155-bd87-ae78a1467670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    result = conversation_chain.invoke({\"question\": message})\n",
    "    return result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b252d8c1-61a8-406d-b57a-8f708a62b014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "view = gr.ChatInterface(chat, type=\"messages\").launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e0ca52-d85e-435d-ad37-3e8138820aae",
   "metadata": {},
   "source": [
    "# Implementing RAGAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6cf7c169-a886-451d-a25a-844845ace347",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ragas in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (0.3.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from ragas) (1.26.4)\n",
      "Requirement already satisfied: datasets in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from ragas) (3.5.1)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from ragas) (0.9.0)\n",
      "Requirement already satisfied: langchain in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from ragas) (0.3.25)\n",
      "Requirement already satisfied: langchain-core in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from ragas) (0.3.74)\n",
      "Requirement already satisfied: langchain-community in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from ragas) (0.3.23)\n",
      "Requirement already satisfied: langchain_openai in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from ragas) (0.3.16)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from ragas) (1.6.0)\n",
      "Requirement already satisfied: appdirs in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from ragas) (1.4.4)\n",
      "Requirement already satisfied: pydantic>=2 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from ragas) (2.11.3)\n",
      "Requirement already satisfied: openai>1 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from ragas) (1.77.0)\n",
      "Requirement already satisfied: diskcache>=5.6.3 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from ragas) (5.6.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from openai>1->ragas) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from openai>1->ragas) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from openai>1->ragas) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from openai>1->ragas) (0.9.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from openai>1->ragas) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from openai>1->ragas) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from openai>1->ragas) (4.13.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from anyio<5,>=3.5.0->openai>1->ragas) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from httpx<1,>=0.23.0->openai>1->ragas) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from httpx<1,>=0.23.0->openai>1->ragas) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>1->ragas) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from pydantic>=2->ragas) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from pydantic>=2->ragas) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from pydantic>=2->ragas) (0.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from tqdm>4->openai>1->ragas) (0.4.6)\n",
      "Requirement already satisfied: filelock in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from datasets->ragas) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from datasets->ragas) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from datasets->ragas) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from datasets->ragas) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from datasets->ragas) (2.32.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from datasets->ragas) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from datasets->ragas) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->ragas) (2025.3.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from datasets->ragas) (3.11.18)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from datasets->ragas) (0.34.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from datasets->ragas) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from datasets->ragas) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from aiohttp->datasets->ragas) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from aiohttp->datasets->ragas) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from aiohttp->datasets->ragas) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from aiohttp->datasets->ragas) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from aiohttp->datasets->ragas) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from aiohttp->datasets->ragas) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from aiohttp->datasets->ragas) (1.20.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from requests>=2.32.2->datasets->ragas) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from requests>=2.32.2->datasets->ragas) (2.4.0)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from langchain->ragas) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from langchain->ragas) (0.3.45)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from langchain->ragas) (2.0.40)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from langchain-core->ragas) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from langchain-core->ragas) (1.33)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core->ragas) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain->ragas) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain->ragas) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain->ragas) (0.23.0)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain->ragas) (3.2.1)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from langchain-community->ragas) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from langchain-community->ragas) (2.9.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from langchain-community->ragas) (0.4.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (0.9.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community->ragas) (1.1.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (1.1.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from tiktoken->ragas) (2024.11.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from pandas->datasets->ragas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from pandas->datasets->ragas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from pandas->datasets->ragas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\goutamsahu\\anaconda3\\envs\\llms\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets->ragas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5435b2b9-935c-48cd-aaf3-73a837ecde49",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q ragas datasets pandas tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac03aa8d-ee1d-4d14-917e-543d3e7f33c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "    answer_correctness,\n",
    ")\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace, HuggingFaceEmbeddings\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f853b2-689b-4c06-b7f2-7c36dcbddc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9df5c0e3-1239-490f-8ae6-0474a7c52376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using RAGAS_MODEL: mistralai/Mistral-7B-Instruct-v0.3\n"
     ]
    }
   ],
   "source": [
    "assert retriever is not None, \"retriever must be defined from previous RAG pipeline\"\n",
    "\n",
    "# Use Hugging Face Inference API\n",
    "RAGAS_MODEL = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "print(f\"Using RAGAS_MODEL: {RAGAS_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118aeabc-0692-4958-81a3-381e7be2a5a5",
   "metadata": {},
   "source": [
    "### Building LLM Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ccc40744-ee54-4fb5-ae2c-4aa7e9b193aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm():\n",
    "    endpoint = HuggingFaceEndpoint(\n",
    "        repo_id=RAGAS_MODEL,\n",
    "        task=\"conversational\",   # important for Mistral-Instruct\n",
    "        temperature=0.0,\n",
    "        max_new_tokens=512,\n",
    "        huggingfacehub_api_token=os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "    )\n",
    "    return ChatHuggingFace(llm=endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c286126d-cc60-487a-bfc1-93353d7aa907",
   "metadata": {},
   "source": [
    "### Creating a new evaluation chain with source documents returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "df8407d6-c09c-45bb-a13e-66492536c4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_eval_chain() -> ConversationalRetrievalChain:\n",
    "    llm = get_llm()\n",
    "    memory = ConversationBufferMemory(\n",
    "        memory_key=\"chat_history\",\n",
    "        return_messages=True,\n",
    "        output_key=\"answer\"\n",
    "    )\n",
    "    return ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=retriever,\n",
    "        memory=memory,\n",
    "        return_source_documents=True,\n",
    "        output_key=\"answer\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8772784-cbc3-4a5e-9b9a-4eaa649a63d6",
   "metadata": {},
   "source": [
    "### Running RAG generation and collecting answers and contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "286355a6-0c48-4dc0-9c12-b82d29304ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_generation(questions: List[str], ground_truth: Optional[List[str]] = None) -> List[Dict]:\n",
    "    results = []\n",
    "    failures = 0\n",
    "    for i, q in enumerate(tqdm(questions, desc=\"Generating answers\")):\n",
    "        try:\n",
    "            chain = make_eval_chain()\n",
    "            output = chain.invoke({\"question\": q})\n",
    "            answer = output.get(\"answer\", \"\").strip()\n",
    "            docs = output.get(\"source_documents\", [])\n",
    "            contexts = list({doc.page_content.strip()[:2000] for doc in docs if doc.page_content.strip()})\n",
    "        except Exception as e:\n",
    "            print(f\"Generation failed for Q{i}: {e}\")\n",
    "            answer, contexts = \"\", []\n",
    "            failures += 1\n",
    "        row = {\n",
    "            \"question\": q,\n",
    "            \"answer\": answer,\n",
    "            \"contexts\": contexts,\n",
    "        }\n",
    "        if ground_truth:\n",
    "            row[\"ground_truth\"] = ground_truth[i]\n",
    "        results.append(row)\n",
    "    print(f\"Evaluated {len(questions)} questions with {failures} failures. Avg contexts: {sum(len(r['contexts']) for r in results)/len(results):.2f}\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb82ddf-1dcc-4812-a47c-b5b09e66b9d7",
   "metadata": {},
   "source": [
    "### Converting rows to RAGAS compatible HuggingFace dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "90db96c4-dd74-416b-b737-8e7733090d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ragas_dataset(rows: List[Dict]) -> Dataset:\n",
    "    keys = [\"question\", \"answer\", \"contexts\"]\n",
    "    if all(\"ground_truth\" in r for r in rows):\n",
    "        keys.append(\"ground_truth\")\n",
    "    return Dataset.from_dict({k: [r.get(k, \"\") for r in rows] for k in keys})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbe9ac7-a3b8-4a97-b477-7ad3a2dc152b",
   "metadata": {},
   "source": [
    "### Running RAGAS evaluation and returning metrics DataFrame and summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0af54067-b36d-4c56-b054-61d67d5d9d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ragas(ds: Dataset) -> Tuple[pd.DataFrame, Dict]:\n",
    "    metrics = [faithfulness, answer_relevancy, context_precision, context_recall]\n",
    "    if \"ground_truth\" in ds.column_names:\n",
    "        metrics.append(answer_correctness)\n",
    "    \n",
    "    ragas_llm = get_llm()   # use Mistral via HF API\n",
    "    ragas_embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "    result = evaluate(ds, metrics=metrics, llm=ragas_llm, embeddings=ragas_embeddings)\n",
    "    df = result.to_pandas()\n",
    "    summary = {metric: round(df[metric].mean(skipna=True), 3) for metric in df.columns if df[metric].dtype != \"O\"}\n",
    "    return df, summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d17cf91-12b9-4d6b-8e1d-8180711565d1",
   "metadata": {},
   "source": [
    "### Save evaluation results to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "112863d6-c05e-40f6-93ab-33e277fd6369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_outputs(df: pd.DataFrame, summary: Dict, path: str = \"./eval\") -> None:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    df.to_csv(f\"{path}/results_rows.csv\", index=False)\n",
    "    with open(f\"{path}/summary.json\", \"w\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    print(f\"Saved results to {path}/results_rows.csv and summary to {path}/summary.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbb818d-dd71-40e2-88ba-33d0897e13b1",
   "metadata": {},
   "source": [
    "### Example questions and ground-truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3bb2633c-160e-4fae-a5c1-741989058982",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"What is Insurellm?\",\n",
    "    \"How does the claims process work?\",\n",
    "    \"What are the benefits of using this platform?\",\n",
    "    \"Can you explain the underwriting model?\",\n",
    "    \"Is there support for multi-language documents?\"\n",
    "]\n",
    "\n",
    "ground_truth = [\n",
    "    \"Insurellm is a platform for insurance-related language model tasks.\",\n",
    "    \"Claims are processed by extracting structured data from documents.\",\n",
    "    \"Benefits include automation, accuracy, and scalability.\",\n",
    "    \"The underwriting model uses AI to assess risk based on documents.\",\n",
    "    \"Yes, the platform supports multilingual document ingestion.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7509a324-33c6-4622-b507-e79ee5b85c8b",
   "metadata": {},
   "source": [
    "### Running The Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c81c5066-1640-49fd-875b-5cd6255667c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers: 100%|██████████| 5/5 [00:07<00:00,  1.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated 5 questions with 0 failures. Avg contexts: 4.00\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fb1d30287dd4391b0eb1c4856fd44fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[7]: ClientResponseError(402, message='Payment Required', url='https://router.huggingface.co/together/v1/chat/completions')\n",
      "Exception raised in Job[12]: ClientResponseError(402, message='Payment Required', url='https://router.huggingface.co/together/v1/chat/completions')\n",
      "Exception raised in Job[16]: ClientResponseError(402, message='Payment Required', url='https://router.huggingface.co/together/v1/chat/completions')\n",
      "Exception raised in Job[13]: ClientResponseError(402, message='Payment Required', url='https://router.huggingface.co/together/v1/chat/completions')\n",
      "Exception raised in Job[10]: ClientResponseError(402, message='Payment Required', url='https://router.huggingface.co/together/v1/chat/completions')\n",
      "Exception raised in Job[9]: ClientResponseError(402, message='Payment Required', url='https://router.huggingface.co/together/v1/chat/completions')\n",
      "Exception raised in Job[11]: ClientResponseError(402, message='Payment Required', url='https://router.huggingface.co/together/v1/chat/completions')\n",
      "Exception raised in Job[4]: ClientResponseError(402, message='Payment Required', url='https://router.huggingface.co/together/v1/chat/completions')\n",
      "Exception raised in Job[6]: ClientResponseError(402, message='Payment Required', url='https://router.huggingface.co/together/v1/chat/completions')\n",
      "Exception raised in Job[1]: ClientResponseError(402, message='Payment Required', url='https://router.huggingface.co/together/v1/chat/completions')\n",
      "Exception raised in Job[14]: ClientResponseError(402, message='Payment Required', url='https://router.huggingface.co/together/v1/chat/completions')\n",
      "Exception raised in Job[0]: ClientResponseError(402, message='Payment Required', url='https://router.huggingface.co/together/v1/chat/completions')\n",
      "Exception raised in Job[17]: ClientResponseError(402, message='Payment Required', url='https://router.huggingface.co/together/v1/chat/completions')\n",
      "Exception raised in Job[3]: ClientResponseError(402, message='Payment Required', url='https://router.huggingface.co/together/v1/chat/completions')\n",
      "Exception raised in Job[5]: ClientResponseError(402, message='Payment Required', url='https://router.huggingface.co/together/v1/chat/completions')\n",
      "Exception raised in Job[18]: ClientResponseError(402, message='Payment Required', url='https://router.huggingface.co/together/v1/chat/completions')\n",
      "Exception raised in Job[21]: ClientResponseError(402, message='Payment Required', url='https://router.huggingface.co/together/v1/chat/completions')\n",
      "Exception raised in Job[19]: ClientResponseError(402, message='Payment Required', url='https://router.huggingface.co/together/v1/chat/completions')\n",
      "Exception raised in Job[23]: ClientResponseError(402, message='Payment Required', url='https://router.huggingface.co/together/v1/chat/completions')\n",
      "Exception raised in Job[22]: TimeoutError()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>answer_correctness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is Insurellm?</td>\n",
       "      <td>[# About Insurellm\\n\\nInsurellm was founded by...</td>\n",
       "      <td>Insurellm is an innovative insurance tech firm...</td>\n",
       "      <td>Insurellm is a platform for insurance-related ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How does the claims process work?</td>\n",
       "      <td>[# Contract with GreenField Holdings for Marke...</td>\n",
       "      <td>The claims process with Homellm works through ...</td>\n",
       "      <td>Claims are processed by extracting structured ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the benefits of using this platform?</td>\n",
       "      <td>[---\\n\\n## Features\\n\\n- **AI-Powered Risk Ass...</td>\n",
       "      <td>The benefits of using this platform include:\\n...</td>\n",
       "      <td>Benefits include automation, accuracy, and sca...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Can you explain the underwriting model?</td>\n",
       "      <td>[# Contract with GreenField Holdings for Marke...</td>\n",
       "      <td>The underwriting model in the context provided...</td>\n",
       "      <td>The underwriting model uses AI to assess risk ...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Is there support for multi-language documents?</td>\n",
       "      <td>[- **User-Friendly Interface**: Designed with ...</td>\n",
       "      <td>Yes, there is a plan to enhance the AI custome...</td>\n",
       "      <td>Yes, the platform supports multilingual docume...</td>\n",
       "      <td>0.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.149856</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       user_input  \\\n",
       "0                              What is Insurellm?   \n",
       "1               How does the claims process work?   \n",
       "2   What are the benefits of using this platform?   \n",
       "3         Can you explain the underwriting model?   \n",
       "4  Is there support for multi-language documents?   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  [# About Insurellm\\n\\nInsurellm was founded by...   \n",
       "1  [# Contract with GreenField Holdings for Marke...   \n",
       "2  [---\\n\\n## Features\\n\\n- **AI-Powered Risk Ass...   \n",
       "3  [# Contract with GreenField Holdings for Marke...   \n",
       "4  [- **User-Friendly Interface**: Designed with ...   \n",
       "\n",
       "                                            response  \\\n",
       "0  Insurellm is an innovative insurance tech firm...   \n",
       "1  The claims process with Homellm works through ...   \n",
       "2  The benefits of using this platform include:\\n...   \n",
       "3  The underwriting model in the context provided...   \n",
       "4  Yes, there is a plan to enhance the AI custome...   \n",
       "\n",
       "                                           reference  faithfulness  \\\n",
       "0  Insurellm is a platform for insurance-related ...           NaN   \n",
       "1  Claims are processed by extracting structured ...           NaN   \n",
       "2  Benefits include automation, accuracy, and sca...           NaN   \n",
       "3  The underwriting model uses AI to assess risk ...          1.00   \n",
       "4  Yes, the platform supports multilingual docume...          0.75   \n",
       "\n",
       "   answer_relevancy  context_precision  context_recall  answer_correctness  \n",
       "0               NaN               0.25             NaN                 NaN  \n",
       "1               NaN                NaN             0.0                 NaN  \n",
       "2               NaN                NaN             NaN                 NaN  \n",
       "3               NaN                NaN             NaN                 NaN  \n",
       "4               NaN                NaN             NaN            0.149856  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rows = run_generation(questions, ground_truth)\n",
    "ds = build_ragas_dataset(rows)\n",
    "df, summary = run_ragas(ds)\n",
    "\n",
    "print(\"Sample results:\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "221ee662-9a52-4d91-9031-177d080bad88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Macro metric means:\n",
      "faithfulness: 0.875\n",
      "answer_relevancy: nan\n",
      "context_precision: 0.250\n",
      "context_recall: 0.000\n",
      "answer_correctness: 0.150\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nMacro metric means:\")\n",
    "for k, v in summary.items():\n",
    "    print(f\"{k}: {v:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5152262-de1c-4d6d-af96-790bc9eac89f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:llms]",
   "language": "python",
   "name": "conda-env-llms-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
